import numpy as np
import cv2
import random
from sklearn.datasets import make_spd_matrix
from scipy.stats import multivariate_normal
from imutils import contours
from generateData import *


# Initialization of weights, means and co-variances
def param_initialize(points_list, d, k):
    weights = np.ones(k) / k
    means = random.choices(points_list, k=k)
    covars = []
    for i in range(k):
        covars.append(make_spd_matrix(d))
    covars = np.array(covars)

    return weights, means, covars


# Computing the pdf of N-dimensional gaussian
def gauss_pdf(data, mean, covar):
    pdf = []
    for x in data:
        d = x.size  # Dimension of the vector
        x = x.reshape((1, d))  # Vector Form
        mean = mean.reshape((1, d))
        x_m = x - mean  # Difference between point and  mean
        frac = 1. / (np.sqrt((2 * np.pi) ** d * np.linalg.det(covar)))
        expo = np.exp(-1 / 2 * np.dot(np.dot(x_m, np.linalg.inv(covar)), x_m.T))
        pdf = frac * expo
        pdf += pdf

    return pdf


# Computing the gaussian mean model with expectation-maximization
def gmm_em(data_file, K):

    data = np.load(data_file)  # Importing data to train

    # Threshold and iterations
    bound = 0.01
    max_itr = 500
    itr = 0

    # Getting initial parameters
    weights, means, covars = param_initialize(data, data[0].size, K)
    # pdf = gauss_pdf(data, means, covars)
    # pdf = np.array(pdf)

    # Running GMM for 500 iterations
    while itr < max_itr:
        itr += 1

        likelihood = []
        # Expectation step
        for j in range(K):
            # if not np.linalg.det(covars[j]) == 0:
                likelihood.append(multivariate_normal.pdf(x=data, mean=means[j], cov=covars[j], allow_singular=True))
        likelihood = np.array(likelihood)
        assert likelihood.shape == (K, len(data))

        # Maximization step (Updating means, co-variances, weights)
        means_previous_list = means.copy()  # store the previous means

        b = []
        eps = 1e-8
        # Maximization step (Updating means, co-variances, weights)
        for j in range(K):
            # Use the current values for the parameters to evaluate the posterior
            # Probabilities of the data to have been generated by each gaussian
            b.append(
                (likelihood[j] * weights[j]) / (np.sum([likelihood[i] * weights[i] for i in range(K)], axis=0) + eps))

            # Update mean and variance
            means[j] = np.sum(b[j].reshape(len(data), 1) * data, axis=0) / (np.sum(b[j] + eps))
            covars[j] = np.dot((b[j].reshape(len(data), 1) * (data - means[j])).T, (data - means[j])) / \
                        (np.sum(b[j]) + eps)

            # Update the weights
            weights[j] = np.mean(b[j])

            assert covars.shape == (K, data.shape[1], data.shape[1])
            # assert means.shape == (K, data.shape[1])

        # Convergence Check
        convergence = sum(
            [np.linalg.norm(mean - mean_previous) for mean, mean_previous in zip(means, means_previous_list)])
        if convergence > bound:
            continue
        else:
            print("Converged")
            break

    return means, covars, weights, K


def loglkhd(image, mu, sigma, weights, nx, ny, div, K):
    likelihoods = np.zeros((K, nx * ny))
    prob = np.zeros(nx * ny)
    for k in range(K):
        likelihoods[k] = weights[k] * multivariate_normal.pdf(image, mu[k], sigma[k], allow_singular=True)
        prob = likelihoods.sum(0)
    prob = np.reshape(prob, (nx, ny))
    prob[prob > np.max(prob) / div] = 255

    return prob

def detect():
    mu_r, sigma_r, weights_r, K_r = gmm_em('Train_red.npy', 3)

    vid = "detectbuoy.avi"
    cap = cv2.VideoCapture(vid)
    images = []
    while (cap.isOpened()):
        success, frame = cap.read()
        if success == False:
            break

        nx, ny, ch = frame.shape
        image = np.reshape(frame, (nx * ny, ch))

        prob_r = np.zeros_like(frame)
        prob_r[:, :, 0] = loglkhd(image, mu_r, sigma_r, weights_r, nx, ny, div=5, K=3)
        prob_r[:, :, 1] = loglkhd(image, mu_r, sigma_r, weights_r, nx, ny, div=5, K=3)
        prob_r[:, :, 2] = loglkhd(image, mu_r, sigma_r, weights_r, nx, ny, div=5, K=3)

        blur_r = cv2.GaussianBlur(prob_r, (3, 3), 20)
        edge_r = cv2.Canny(blur_r, 20, 255)
        cnts_r, _ = cv2.findContours(edge_r, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
        cnts_sorted_r, _ = contours.sort_contours(cnts_r, method="left-to-right")
        hull_r = cv2.convexHull(cnts_sorted_r[0])
        (x_r, y_r), rad_r = cv2.minEnclosingCircle(hull_r)

        YR = np.real(np.sqrt((x_r) ^ 2 + (y_r) ^ 2))

        if rad_r > 7 and YR > 6:
            cv2.circle(frame, (int(x_r), int(y_r)), int(rad_r), (0, 128, 255), 4)

            images.append(frame)

        else:
            images.append(frame)

        cv2.imshow("output", frame)
        cv2.waitKey(1)

    cap.release()


if __name__ == "__main__":
    detect()